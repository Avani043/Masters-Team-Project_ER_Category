{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mcC3mZrUSYY","outputId":"3622e8a2-6aca-4b80-8b8d-9b27bedefe91","executionInfo":{"status":"ok","timestamp":1738178095192,"user_tz":-60,"elapsed":241775,"user":{"displayName":"jahanvi panchal","userId":"04637032965642579762"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n","Requirement already satisfied: nlpaug in /usr/local/lib/python3.11/dist-packages (1.1.11)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.6)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n","Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:21<00:00,  2.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.9994\n","Validation F1 Score: 0.4460\n","Epoch 2/10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:21<00:00,  2.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.8709\n","Validation F1 Score: 0.4989\n","Epoch 3/10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:20<00:00,  2.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7006\n","Validation F1 Score: 0.5076\n","Epoch 4/10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:21<00:00,  2.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.5561\n","Validation F1 Score: 0.4941\n","Epoch 5/10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:21<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.4248\n","Validation F1 Score: 0.4864\n","Epoch 6/10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:20<00:00,  2.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3328\n","Validation F1 Score: 0.4860\n","Epoch 7/10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:20<00:00,  2.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2733\n","Validation F1 Score: 0.4792\n","Epoch 8/10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:20<00:00,  2.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2309\n","Validation F1 Score: 0.4739\n","Epoch 9/10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:21<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2086\n","Validation F1 Score: 0.4926\n","Epoch 10/10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:20<00:00,  2.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.1968\n","Validation F1 Score: 0.4831\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-ef710458b3ab>:235: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(\"best_model_augmented.pth\"))\n"]},{"output_type":"stream","name":"stdout","text":["Final Evaluation on Test Set (After Data Augmentation)\n","Classification Report:\n","                        precision    recall  f1-score   support\n","\n","Attentional Deployment       0.42      0.63      0.50        51\n","      Cognitive Change       0.49      0.75      0.59        56\n","   Response Modulation       0.20      0.70      0.31        27\n","Situation Modification       0.27      0.46      0.34        13\n","   Situation Selection       0.16      0.57      0.25        14\n","                  none       0.68      0.79      0.73        19\n","              adaptive       0.74      0.87      0.80        92\n","           maladaptive       0.44      0.67      0.53        27\n","\n","             micro avg       0.44      0.74      0.55       299\n","             macro avg       0.43      0.68      0.51       299\n","          weighted avg       0.51      0.74      0.59       299\n","           samples avg       0.44      0.74      0.54       299\n","\n"]}],"source":["# Necessary Libraries\n","!pip install pandas tensorflow transformers scikit-learn numpy matplotlib seaborn imbalanced-learn openpyxl nlpaug\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import random\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, f1_score\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import AdamW, get_scheduler\n","from tqdm import tqdm\n","import torch.nn as nn\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('averaged_perceptron_tagger_eng')\n","import nlpaug.augmenter.word as naw\n","\n","# Set Random Seed\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n","\n","# Dataset Class\n","class ERDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer(\n","            text,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors=\"pt\",\n","        )\n","\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n","            \"labels\": torch.tensor(label, dtype=torch.float),\n","        }\n","\n","# Load Dataset\n","file_path = \"/content/drive/MyDrive/Dataset500.xlsx\"\n","df = pd.read_excel(file_path)\n","\n","# Filter out rows with missing values in `er_strat`\n","df = df[df['er_strat'].notna()]\n","\n","# Process Labels\n","# Multi-label binarization for `er_strat` (6 categories)\n","mlb = MultiLabelBinarizer()\n","df['er_labels'] = df['er_strat'].apply(lambda x: x.split(\", \"))\n","er_labels = mlb.fit_transform(df['er_labels'])\n","\n","# Process `adaptive` as two binary labels: adaptive (1, 0) and maladaptive (0, 1)\n","df['adaptive_binary'] = df['adaptive'].apply(\n","    lambda x: [1, 0] if x == \"adaptive\" else [0, 1] if x == \"maladaptive\" else [0, 0]\n",")\n","adaptive_labels = np.array(df['adaptive_binary'].tolist())\n","\n","# Combine all labels: 6 `er_strat` + 2 `adaptive/maladaptive`\n","combined_labels = np.hstack([er_labels, adaptive_labels])\n","\n","# Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['text_cleaned'], combined_labels, test_size=0.3, random_state=42\n",")\n","\n","# Tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","# Data Augmentation for Weak Classes\n","def augment_text(text, augmenter, num_augments=2):\n","    \"\"\"\n","    Augment a given text using the specified augmenter.\n","    :param text: Input text to augment.\n","    :param augmenter: Augmentation method (e.g., synonym replacement).\n","    :param num_augments: Number of augmented samples to generate.\n","    :return: List of augmented texts.\n","    \"\"\"\n","    augmented_texts = [augmenter.augment(text) for _ in range(num_augments)]\n","    return augmented_texts\n","\n","# Identify weak classes\n","weak_classes = [\"Situation Modification\", \"Response Modulation\", \"Situation Selection\"]\n","\n","# Get indices of samples belonging to weak classes\n","weak_class_indices = [i for i, label in enumerate(y_train) if any(label[mlb.classes_.tolist().index(cls)] for cls in weak_classes)]\n","\n","# Augment samples for weak classes\n","augmented_texts = []\n","augmented_labels = []\n","\n","# Synonym Replacement Augmenter\n","synonym_augmenter = naw.SynonymAug(aug_src='wordnet')\n","\n","for idx in weak_class_indices:\n","    text = X_train.iloc[idx]\n","    label = y_train[idx]\n","\n","    # Generate augmented texts\n","    new_texts = augment_text(text, synonym_augmenter, num_augments=5)\n","\n","    # Add augmented texts and their corresponding labels\n","    augmented_texts.extend(new_texts)\n","    augmented_labels.extend([label] * len(new_texts))\n","\n","# Add augmented data to the training set\n","X_train = X_train.tolist() + augmented_texts\n","y_train = np.vstack([y_train, augmented_labels])\n","\n","# Dataset and DataLoader\n","max_length = 128\n","batch_size = 16\n","\n","train_dataset = ERDataset(X_train, y_train, tokenizer, max_length)\n","test_dataset = ERDataset(X_test.tolist(), y_test, tokenizer, max_length)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Model\n","num_labels = combined_labels.shape[1]  # Total number of labels (6 `er_strat` + 2 `adaptive`)\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",")\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n","\n","# Loss Function with Class Weights\n","class_counts = np.sum(y_train, axis=0)\n","epsilon = 1e-5  # To avoid division by zero\n","pos_weights = torch.tensor(\n","    (len(y_train) - class_counts) / (class_counts + epsilon), dtype=torch.float\n",").to(device)\n","loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n","\n","# Optimizer and Scheduler\n","epochs = 10\n","optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n","num_training_steps = len(train_loader) * epochs\n","num_warmup_steps = int(0.1 * num_training_steps)\n","scheduler = get_scheduler(\n","    \"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",")\n","\n","# Training Function\n","def train_model(model, data_loader, optimizer, loss_fn, scheduler, device):\n","    model.train()\n","    total_loss = 0\n","\n","    for batch in tqdm(data_loader):\n","        optimizer.zero_grad()\n","\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","        )\n","\n","        loss = loss_fn(outputs.logits, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n","        optimizer.step()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(data_loader)\n","\n","# Evaluation Function\n","def evaluate_model(model, data_loader, device, threshold=0.5):\n","    model.eval()\n","    predictions, true_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask\n","            )\n","\n","            logits = outputs.logits\n","            predictions.extend(torch.sigmoid(logits).cpu().numpy())\n","            true_labels.extend(batch['labels'].cpu().numpy())\n","\n","    binary_predictions = (np.array(predictions) > threshold).astype(int)\n","    return binary_predictions, np.array(true_labels)\n","\n","# Training Loop\n","best_f1 = 0\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch + 1}/{epochs}\")\n","    train_loss = train_model(model, train_loader, optimizer, loss_fn, scheduler, device)\n","    print(f\"Train Loss: {train_loss:.4f}\")\n","\n","    # Evaluate on validation data\n","    predictions, true_labels = evaluate_model(model, test_loader, device)\n","    f1 = f1_score(true_labels, predictions, average=\"macro\")\n","    print(f\"Validation F1 Score: {f1:.4f}\")\n","\n","    # Save best model\n","    if f1 > best_f1:\n","        best_f1 = f1\n","        torch.save(model.state_dict(), \"best_model_augmented.pth\")\n","\n","# Final Evaluation\n","model.load_state_dict(torch.load(\"best_model_augmented.pth\"))\n","predictions, true_labels = evaluate_model(model, test_loader, device)\n","\n","# Classification Report\n","print(\"Final Evaluation on Test Set (After Data Augmentation)\")\n","print(\"Classification Report:\")\n","target_names = list(mlb.classes_) + [\"adaptive\", \"maladaptive\"]\n","print(classification_report(true_labels, predictions, target_names=target_names))"]},{"cell_type":"code","source":["# Print the number of augmented samples\n","print(f\"Number of augmented texts: {len(augmented_texts)}\")\n","print(f\"Number of augmented labels: {len(augmented_labels)}\")\n","\n","# Print some examples of augmented texts and their labels\n","for i in range(min(5, len(augmented_texts))):  # Print first 5 examples\n","    print(f\"Augmented Text {i+1}: {augmented_texts[i]}\")\n","    print(f\"Corresponding Label: {augmented_labels[i]}\")\n","    print(\"-\" * 50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8wqFwz6kcxzy","executionInfo":{"status":"ok","timestamp":1738178199728,"user_tz":-60,"elapsed":483,"user":{"displayName":"jahanvi panchal","userId":"04637032965642579762"}},"outputId":"0df87529-0a60-4630-b5f7-d347118df884"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of augmented texts: 560\n","Number of augmented labels: 560\n","Augmented Text 1: ['Right now i feel so low. Apr 4. I donâ € ™ t quite now why. All of it i guess. Iâ € ™ m not excited just about much of anything, even the thought of home decor. Oregon the possibility of a new patio and hosting, with a few unintentional moments of feeling left out from [Named Entity] and [Named Entity] and [Named Entity] and [Named Entity ]. Single want to include everyone but that takes vim. I donâ € ™ t have much. Thankfully God has been sending me Orr meetings. . Iâ € ™ m working with [Named Entity] of them this week. It gets me out of my head. Iâ € ™ m trying to get excited about planning VBS with [Named Entity ], about virtual bible study with [Named Entity] and [Named Entity ], about spelling. About exercise. About anything. I want uncluttered time with God to communicate and hear from him and he told me that that isnâ € ™ t always possible, that life IS cluttered, and Iâ € ™ m to find him and talk to him through with all of that. [Named Entity] has spring break next week which i hope will be a good matter.']\n","Corresponding Label: [0 1 0 0 1 0 1 0]\n","--------------------------------------------------\n","Augmented Text 2: ['Right now i feel so low. Apr 4. I donâ € ™ t quite now why. All of it i guess. Iâ € ™ m not excited about much of anything, even the thought of home decor. Or the possibility of a new patio and hosting, with a few unintentional moments of feeling left out from [Named Entity] and [Named Entity] and [Named Entity] and [Named Entity ]. I want to include everyone but that takes energy. I donâ € ™ t have much. Thankfully God has been sending me Orr meetings. . Iâ € ™ m working with [Named Entity] of them this week. It gets me out of my head. Iâ € ™ m trying to get excited about planning VBS with [Named Entity ], about virtual bible study with [Named Entity] and [Named Entity ], about spell. About exercise. About anything. I want uncluttered time with Supreme being to communicate and listen from him and he told me that that isnâ € ™ mt always possible, that life IS cluttered, and Iâ € ™ m to find him and talk to him through all of that. [Named Entity] has spring breaking next week which i hope will be a good thing.']\n","Corresponding Label: [0 1 0 0 1 0 1 0]\n","--------------------------------------------------\n","Augmented Text 3: ['Right now i feel so low. April 4. I donâ € ™ t quite now why. All of it i guess. Iâ € ™ metre not excited around much of anything, even the thought of home decor. Or the possibility of a new terrace and hosting, with a few unintentional moments of feeling left out from [Named Entity] and [Named Entity] and [Named Entity] and [Named Entity ]. I want to include everyone but that takes energy. Ace donâ € ™ t have much. Thankfully God has been sending me Orr meetings. . Iâ € ™ m working with [Named Entity] of them this week. It gets me out of my head. Iâ € ™ m essay to get excited about planning VBS with [Named Entity ], about virtual bible study with [Named Entity] and [Named Entity ], about spelling. About exercise. About anything. I want uncluttered time with God to communicate and hear from him and he told me that that isnâ € ™ t always possible, that life IS cluttered, and Iâ € ™ m to find him and talk to him through all of that. [Named Entity] has springiness break next week which i hope will be a good thing.']\n","Corresponding Label: [0 1 0 0 1 0 1 0]\n","--------------------------------------------------\n","Augmented Text 4: ['Right now i feel so low. April 4. I donâ € ™ t quite now why. All of it i guess. Iâ € ™ m not excited about much of anything, even the cerebration of home decor. Or the possibility of a new patio and hosting, with a few unintentional moments of feeling left out from [Named Entity] and [Named Entity] and [Named Entity] and [Named Entity ]. I want to include everyone but that takes energy. I donâ € ™ thymine have much. Thankfully God has been sending me Orr meetings. . Iâ € ™ m working with [Named Entity] of them this calendar week. It gets me out of my head. Iâ € ™ m trying to get excited about planning VBS with [Named Entity ], about virtual bible study with [Named Entity] and [Named Entity ], about spelling. About exercise. About anything. I want uncluttered metre with God to pass and hear from him and he told me that that isnâ € ™ t always possible, that life history IS clutter, and Iâ € ™ m to find him and talk to him through all of that. [Named Entity] has spring break next week which i hope will comprise a good thing.']\n","Corresponding Label: [0 1 0 0 1 0 1 0]\n","--------------------------------------------------\n","Augmented Text 5: ['Right now i feel so low. April 4. I donâ € ™ t quite now why. All of it i guess. Iâ € ™ m not excite about much of anything, even the thought of home decor. Or the possibility of a new patio and hosting, with a few unintentional moments of feeling left out from [Named Entity] and [Named Entity] and [Named Entity] and [Named Entity ]. I want to include everyone but that takes energy. One donâ € ™ t have much. Thankfully God has been sending me Orr meetings. . Iâ € ™ m working with [Named Entity] of them this week. Information technology gets me out of my head. Iâ € ™ m trying to get excited about planning VBS with [Named Entity ], about virtual bible study with [Named Entity] and [Named Entity ], about spelling. About exercise. About anything. I want uncluttered time with God to communicate and hear from him and he told me that that isnâ € ™ t always possible, that life IS cluttered, and Iâ € ™ m to find him and talk to him through all of that. [Named Entity] has spring break next week which i hope will be a good thing.']\n","Corresponding Label: [0 1 0 0 1 0 1 0]\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Print the size of the training set before and after augmentation\n","print(f\"Training set size before augmentation: {len(X_train) - len(augmented_texts)}\")\n","print(f\"Training set size after augmentation: {len(X_train)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4STWGHspbIaD","executionInfo":{"status":"ok","timestamp":1738178205815,"user_tz":-60,"elapsed":395,"user":{"displayName":"jahanvi panchal","userId":"04637032965642579762"}},"outputId":"f3558cd5-7607-40d8-a342-d5a33f8d8cf0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set size before augmentation: 336\n","Training set size after augmentation: 896\n"]}]},{"cell_type":"code","source":["print(f\"test set size after augmentation: {len(X_test)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zaV5Fqu3gUTb","executionInfo":{"status":"ok","timestamp":1738178453521,"user_tz":-60,"elapsed":451,"user":{"displayName":"jahanvi panchal","userId":"04637032965642579762"}},"outputId":"70745898-e56e-4306-b4e1-575374f029c9"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set size after augmentation: 144\n"]}]},{"cell_type":"code","source":["# Count samples for weak classes before and after augmentation\n","weak_classes = [\"Situation Modification\", \"Response Modulation\", \"Situation Selection\"]\n","\n","# Before augmentation\n","weak_class_counts_before = {cls: 0 for cls in weak_classes}\n","for label in y_train[:len(y_train) - len(augmented_labels)]:\n","    for cls in weak_classes:\n","        if label[mlb.classes_.tolist().index(cls)] == 1:\n","            weak_class_counts_before[cls] += 1\n","\n","# After augmentation\n","weak_class_counts_after = {cls: 0 for cls in weak_classes}\n","for label in y_train:\n","    for cls in weak_classes:\n","        if label[mlb.classes_.tolist().index(cls)] == 1:\n","            weak_class_counts_after[cls] += 1\n","\n","print(\"Weak class counts before augmentation:\", weak_class_counts_before)\n","print(\"Weak class counts after augmentation:\", weak_class_counts_after)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZDLBYzfdG1w","executionInfo":{"status":"ok","timestamp":1738178211725,"user_tz":-60,"elapsed":508,"user":{"displayName":"jahanvi panchal","userId":"04637032965642579762"}},"outputId":"f277c460-f337-438d-9425-15041ae8409e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Weak class counts before augmentation: {'Situation Modification': 22, 'Response Modulation': 68, 'Situation Selection': 29}\n","Weak class counts after augmentation: {'Situation Modification': 132, 'Response Modulation': 408, 'Situation Selection': 174}\n"]}]}]}